"""
Unit tests to validate UTF-8 encoding in workflow tools
and the generation of properly encoded JSON responses.
"""

import json
import os
import tempfile
import unittest
from unittest.mock import Mock, patch

from tools.analyze import AnalyzeTool
from tools.codereview import CodereviewTool
from tools.debug import DebugIssueTool


class TestWorkflowToolsUTF8(unittest.TestCase):
    """Tests for UTF-8 encoding in workflow tools."""

    def setUp(self):
        """Test setup."""
        self.original_locale = os.getenv("LOCALE")
        # Default to French for tests
        os.environ["LOCALE"] = "fr-FR"

    def tearDown(self):
        """Cleanup after tests."""
        if self.original_locale is not None:
            os.environ["LOCALE"] = self.original_locale
        else:
            os.environ.pop("LOCALE", None)

    def test_workflow_json_response_structure(self):
        """Test the structure of JSON responses from workflow tools."""
        # Test with analysis tool
        analyze_tool = AnalyzeTool()

        # Mock response with UTF-8 characters
        test_response = {
            "status": "pause_for_analysis",
            "step_number": 1,
            "total_steps": 3,
            "next_step_required": True,
            "findings": "Code analysis reveals performance issues üîç",
            "files_checked": ["/src/main.py"],
            "relevant_files": ["/src/main.py"],
            "issues_found": [
                {"severity": "high", "description": "Function too complex - refactoring needed"}
            ],
            "investigation_required": True,
            "required_actions": ["Review code dependencies", "Analyze architectural patterns"],
        }

        # Test JSON serialization with ensure_ascii=False
        json_str = json.dumps(test_response, indent=2, ensure_ascii=False)

        # UTF-8 checks
        self.assertIn("r√©v√®le", json_str)
        self.assertIn("probl√®mes", json_str)
        self.assertIn("n√©cessaire", json_str)
        self.assertIn("d√©pendances", json_str)
        self.assertIn("üîç", json_str)

        # No escaped characters
        self.assertNotIn("\\u", json_str)

        # Test parsing
        parsed = json.loads(json_str)
        self.assertEqual(parsed["findings"], test_response["findings"])
        self.assertEqual(len(parsed["issues_found"]), 1)
        self.assertIn("n√©cessaire", parsed["issues_found"][0]["description"])

    @patch("tools.shared.base_tool.BaseTool.get_model_provider")
    def test_analyze_tool_utf8_response(self, mock_get_provider):
        """Test that the analyze tool returns correct UTF-8 responses."""
        # Mock provider
        mock_provider = Mock()
        mock_provider.get_provider_type.return_value = Mock(value="test")
        mock_provider.generate_content.return_value = Mock(
            content="Architectural analysis complete. Recommendations: improve modularity.",
            usage={},
            model_name="test-model",
            metadata={},
        )
        mock_get_provider.return_value = mock_provider

        # Test the tool
        analyze_tool = AnalyzeTool()
        result = analyze_tool.execute(
            {
                "step": "Analyze system architecture to identify issues",
                "step_number": 1,
                "total_steps": 2,
                "next_step_required": True,
                "findings": "Starting architectural analysis of Python code",
                "relevant_files": ["/test/main.py"],
                "model": "test-model",
            }
        )

        # Checks
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 1)

        # Parse the response - must be valid UTF-8 JSON
        response_text = result[0].text
        response_data = json.loads(response_text)

        # Structure checks
        self.assertIn("status", response_data)
        self.assertIn("step_number", response_data)

        # Check that the French instruction was added
        mock_provider.generate_content.assert_called()
        call_args = mock_provider.generate_content.call_args
        system_prompt = call_args.kwargs.get("system_prompt", "")
        self.assertIn("fr-FR", system_prompt)

    @patch("tools.shared.base_tool.BaseTool.get_model_provider")
    def test_codereview_tool_french_findings(self, mock_get_provider):
        """Test that the codereview tool produces findings in French."""
        # Mock with analysis in French
        mock_provider = Mock()
        mock_provider.get_provider_type.return_value = Mock(value="test")
        mock_provider.supports_thinking_mode.return_value = False
        mock_provider.generate_content.return_value = Mock(
            content=json.dumps(
                {
                    "status": "analysis_complete",
                    "raw_analysis": """
üî¥ CRITIQUE: Aucun probl√®me critique trouv√©.

üü† √âLEV√â: Fichier example.py:42 - Fonction trop complexe
‚Üí Probl√®me: La fonction process_data() contient trop de responsabilit√©s
‚Üí Solution: D√©composer en fonctions plus petites et sp√©cialis√©es

üü° MOYEN: Gestion d'erreurs insuffisante
‚Üí Probl√®me: Plusieurs fonctions n'ont pas de gestion d'erreurs appropri√©e
‚Üí Solution: Ajouter des try-catch et validation des param√®tres

‚úÖ Points positifs:
‚Ä¢ Code bien comment√© et lisible
‚Ä¢ Nomenclature coh√©rente
‚Ä¢ Tests unitaires pr√©sents
""",
                },
                ensure_ascii=False,
            ),
            usage={},
            model_name="test-model",
            metadata={},
        )
        mock_get_provider.return_value = mock_provider

        # Test the tool
        codereview_tool = CodereviewTool()
        result = codereview_tool.execute(
            {
                "step": "Complete review of Python code",
                "step_number": 1,
                "total_steps": 1,
                "next_step_required": False,
                "findings": "Code review complete",
                "relevant_files": ["/test/example.py"],
                "model": "test-model",
            }
        )

        # Checks
        self.assertIsNotNone(result)
        response_text = result[0].text
        response_data = json.loads(response_text)

        # Check UTF-8 characters in analysis
        if "expert_analysis" in response_data:
            analysis = response_data["expert_analysis"]["raw_analysis"]
            # V√©rification de caract√®res fran√ßais
            # Check for French characters
            self.assertIn("√âLEV√â", analysis)is)
            self.assertIn("probl√®me", analysis)sis)
            self.assertIn("sp√©cialis√©es", analysis)
            self.assertIn("appropri√©e", analysis)
            self.assertIn("param√®tres", analysis)
            self.assertIn("pr√©sents", analysis)
            # V√©rification d'emojis
            # Check for emojislysis)
            self.assertIn("üî¥", analysis)
            self.assertIn("üü†", analysis)
            self.assertIn("üü°", analysis)
            self.assertIn("‚úÖ", analysis)
    @patch("tools.shared.base_tool.BaseTool.get_model_provider")
    @patch("tools.shared.base_tool.BaseTool.get_model_provider")vider):
    def test_debug_tool_french_error_analysis(self, mock_get_provider):
        """Test that the debug tool analyzes errors in French."""
        # Mock providerck()
        mock_provider = Mock()ider_type.return_value = Mock(value="test")
        mock_provider.get_provider_type.return_value = Mock(value="test")
        mock_provider.generate_content.return_value = Mock(n d√©finie. Cause probable: import manquant.",
            content="Error analyzed: variable 'donn√©es' not defined. Probable cause: missing import.",
            usage={},e="test-model",
            model_name="test-model",
            metadata={},
        )ock_get_provider.return_value = mock_provider
        mock_get_provider.return_value = mock_provider
        # Test de l'outil debug
        # Test the debug toolTool()
        debug_tool = DebugIssueTool()
        result = debug_tool.execute(
            {   "step": "Analyser l'erreur NameError dans le fichier de traitement des donn√©es",
                "step": "Analyze NameError in data processing file",
                "step_number": 1,
                "total_steps": 2,ed": True,
                "next_step_required": True,e lors de l'ex√©cution du script",
                "findings": "Error detected during script execution",
                "files_checked": ["/src/data_processor.py"],,
                "relevant_files": ["/src/data_processor.py"], - import manquant",
                "hypothesis": "Variable 'donn√©es' not defined - missing import",
                "confidence": "medium",
                "model": "test-model",
            }
        )
        # V√©rifications
        # CheckstNone(result)
        self.assertIsNotNone(result)xt
        response_text = result[0].textponse_text)
        response_data = json.loads(response_text)
        # V√©rification de la structure de r√©ponse
        # Check response structure
        self.assertIn("status", response_data)response_data)
        self.assertIn("investigation_status", response_data)
        # V√©rification que les caract√®res UTF-8 sont pr√©serv√©s
        # Check that UTF-8 characters are preservedFalse)
        response_str = json.dumps(response_data, ensure_ascii=False)
        self.assertIn("donn√©es", response_str))
        self.assertIn("d√©tect√©e", response_str))
        self.assertIn("ex√©cution", response_str)
        self.assertIn("d√©finie", response_str)
    def test_workflow_mixin_utf8_serialization(self):
    def test_workflow_mixin_utf8_serialization(self):lowMixin."""
        """Test UTF-8 serialization in BaseWorkflowMixin."""
        # Simulation of a workflow response with UTF-8 characters
        workflow_response = {g_expert_analysis",
            "status": "calling_expert_analysis",
            "step_number": 2,
            "total_steps": 3,ed": True,
            "next_step_required": True,",
            "continuation_id": "test-id",
            "file_context": {y_embedded",
                "type": "fully_embedded",
                "files_embedded": 2,n": "Contexte optimis√© pour l'analyse experte",
                "context_optimization": "Context optimized for expert analysis",
            },xpert_analysis": {
            "expert_analysis": {sis_complete",
                "status": "analysis_complete",
                "raw_analysis": """
Complete system analysis reveals:
üéØ **Objectif**: Am√©liorer les performances
üéØ **Objective**: Improve performancenamique
üîç **Methodology**: Static and dynamic analysis
üìä **Results**: n√©rale: satisfaisante
  ‚Ä¢ Overall performance: satisfactory√©es
  ‚Ä¢ Possible optimizations: 3 identifiedlog n)
  ‚Ä¢ Algorithmic complexity: O(n¬≤) ‚Üí O(n log n)
**Recommandations prioritaires**:
**Priority recommendations**:es donn√©es
1. Optimize the data sorting functionr√©quentes
2. Implement a cache for frequent requests
3. Refactor the report generation module
üöÄ **Impact attendu**: Am√©lioration de 40% des performances
üöÄ **Expected impact**: 40% improvement in performance
""",        },
            },nvestigation_summary": {
            "investigation_summary": {rc/performance.py", "/src/cache.py"],
                "files_analyzed": ["/src/performance.py", "/src/cache.py"],nt des donn√©es",
                "key_findings": "Optimizations identified in data processing",
                "recommendations": "Implement caching and algorithmic improvement",
            },
        }
        # Test de s√©rialisation avec ensure_ascii=False
        # Test serialization with ensure_ascii=False=2, ensure_ascii=False)
        json_str = json.dumps(workflow_response, indent=2, ensure_ascii=False)
        # V√©rifications de pr√©servation UTF-8
        # UTF-8 preservation checks
        utf8_chars = [
            "r√©v√®le",ogie",
            "M√©thodologie",
            "g√©n√©rale",s",
            "identifi√©es",,
            "prioritaires",
            "donn√©es",s",
            "fr√©quentes",
            "g√©n√©ration",
            "attendu",ion",
            "Am√©lioration",
            "identifi√©es",,
            "am√©lioration",
        ]
        for char_seq in utf8_chars:
        for char_seq in utf8_chars: json_str)
            self.assertIn(char_seq, json_str)
        # V√©rifications d'emojis
        # Emoji checks", "üöÄ"]
        emojis = ["üéØ", "üîç", "üìä", "üöÄ"]
        for emoji in emojis:oji, json_str)
            self.assertIn(emoji, json_str)
        # Pas de caract√®res √©chapp√©s
        # No escaped characters_str)
        self.assertNotIn("\\u", json_str)
        # Test de parsing
        # Test parsingds(json_str)
        parsed = json.loads(json_str)
        self.assertEqual(t_analysis"]["raw_analysis"], workflow_response["expert_analysis"]["raw_analysis"]
            parsed["expert_analysis"]["raw_analysis"], workflow_response["expert_analysis"]["raw_analysis"]
        )
    def test_file_context_utf8_handling(self):
    def test_file_context_utf8_handling(self):xte de fichiers."""
        """Test UTF-8 handling in file context."""
        # Create a temporary file with UTF-8 content
        french_code = '''#!/usr/bin/env python3
"""ule de traitement des donn√©es utilisateur.
Module for processing user data.
Created by: Development Team
"""
class GestionnaireDonn√©es:
class DataHandler:e traitement des donn√©es utilisateur."""
    """Handler for processing user data."""
    def __init__(self):
    def __init__(self):{}
        self.data = {}= {}
        self.preferences = {}
        traiter_donn√©es(self, donn√©es_entr√©e):
    def process_data(self, input_data):
        """ite les donn√©es d'entr√©e selon les pr√©f√©rences.
        Processes input data according to preferences.
        Args:
        Args:onn√©es_entr√©e: Donn√©es √† traiter
            input_data: Data to process
            rns:
        Returns:√©es trait√©es et format√©es
            Processed and formatted data
        """ultat = {}
        result = {}
        for cl√©, valeur in donn√©es_entr√©e.items():
        for key, value in input_data.items():
            if self._validate_data(value):r_donn√©es(valeur)
                result[key] = self._format_data(value)
                √©sultat
        return result
        _valider_donn√©es(self, donn√©es):
    def _validate_data(self, data):es."""
        """Validates the structure of the data."""(donn√©es)) > 0
        return data is not None and len(str(data)) > 0
        _formater_donn√©es(self, donn√©es):
    def _format_data(self, data):r√®gles m√©tier."""
        """Formats the data according to business rules."""
        return f"Formatted: {data}"
# Configuration par d√©faut
# Default configuration
DEFAULT_CONFIG = {utf-8",
    "encoding": "utf-8",,
    "language": "French",aris"
    "timezone": "Europe/Paris"
}
def cr√©er_gestionnaire():
def create_handler():du gestionnaire de donn√©es."""
    """Creates an instance of the data handler."""
    return DataHandler()
'''
        with tempfile.NamedTemporaryFile(mode="w", encoding="utf-8", suffix=".py", delete=False) as f:
        with tempfile.NamedTemporaryFile(mode="w", encoding="utf-8", suffix=".py", delete=False) as f:
            f.write(french_code)
            temp_file = f.name
        try:
        try:# Test de lecture et traitement UTF-8
            # Test reading and processing UTF-8tf-8") as f:
            with open(temp_file, "r", encoding="utf-8") as f:
                content = f.read()
            # Simulation du contexte de fichier pour workflow
            # Simulate file context for workflow
            file_context = { temp_file,
                "file_path": temp_file,
                "content": content,,
                "encoding": "utf-8", Python avec noms de variables en fran√ßais",
                "analysis": "Python file with variable names in French",
                "metrics": { len(content.split("\n")),
                    "lines": len(content.split("\n")),
                    "classes": 1,
                    "methods": 4,p√©ciaux": ["√©", "√®", "√†", "√ß", "√π"],
                    "special_characters": ["√©", "√®", "√†", "√ß", "√π"],
                },
            }
            # Test de s√©rialisation du contexte
            # Test context serializationext, ensure_ascii=False, indent=2)
            context_json = json.dumps(file_context, ensure_ascii=False, indent=2)
            # V√©rifications UTF-8
            # UTF-8 checksnnaireDonn√©es", context_json)
            self.assertIn("DataHandler", context_json)
            self.assertIn("data", context_json)son)
            self.assertIn("preferences", context_json)on)
            self.assertIn("input_data", context_json)n)
            self.assertIn("format_data", context_json)n)
            self.assertIn("create_handler", context_json)
            self.assertIn("French", context_json)
            # Test de parsing
            # Test parsingjson.loads(context_json)
            parsed_context = json.loads(context_json)], content)
            self.assertEqual(parsed_context["content"], content))
            self.assertIn("French", parsed_context["analysis"])
        finally:
        finally:ttoyage
            # Cleanupemp_file)
            os.unlink(temp_file)
    def test_error_response_utf8_format(self):
    def test_error_response_utf8_format(self):les r√©ponses workflow."""
        """Test UTF-8 error format in workflow responses."""
        # Simulation of an error response with UTF-8 characters
        error_response = {or",
            "status": "error",idationError",
            "error_type": "ValidationError",√©e invalides: caract√®res sp√©ciaux non support√©s",
            "error_message": "Invalid input data: unsupported special characters",
            "error_details": {rc/donn√©es.py",
                "file": "/src/donn√©es.py",
                "line": 42,"Encodage UTF-8 requis pour les noms de variables accentu√©es",
                "issue": "UTF-8 encoding required for accented variable names",
                "solution": "Check file encoding and IDE settings",
            },uggestions": [
            "suggestions": [-*- coding: utf-8 -*- en en-t√™te",
                "Use # -*- coding: utf-8 -*- at the top",
                "Set IDE to UTF-8 by default",e",
                "Check system locale settings",
            ],imestamp": "2024-01-01T12:00:00Z",
            "timestamp": "2024-01-01T12:00:00Z",
        }
        # Test de s√©rialisation d'erreur
        # Test error serializationsponse, ensure_ascii=False, indent=2)
        error_json = json.dumps(error_response, ensure_ascii=False, indent=2)
        # V√©rifications UTF-8
        # UTF-8 checkss", error_json)
        self.assertIn("Donn√©es", error_json)
        self.assertIn("entr√©e", error_json)n)
        self.assertIn("sp√©ciaux", error_json))
        self.assertIn("support√©s", error_json))
        self.assertIn("donn√©es.py", error_json)
        self.assertIn("probl√®me", error_json)n)
        self.assertIn("accentu√©es", error_json)
        self.assertIn("V√©rifier", error_json)n)
        self.assertIn("param√®tres", error_json)
        # Test de parsing
        # Test parsingon.loads(error_json)
        parsed_error = json.loads(error_json)type"], "ValidationError")
        self.assertEqual(parsed_error["error_type"], "ValidationError")l√®me"])
        self.assertIn("accentu√©es", parsed_error["error_details"]["probl√®me"])

if __name__ == "__main__":
if __name__ == "__main__":y=2)
    unittest.main(verbosity=2)