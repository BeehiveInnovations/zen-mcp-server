# Tiered Consensus - Phase 2 Implementation Plan

**Date:** 2025-11-09
**Status:** Planning
**Phase 1 Completed:** Architecture, deprecation, documentation

---

## Phase 1 Summary (✅ COMPLETED)

### What We Built:
- **tiered_consensus.py** - Main tool with workflow orchestration
- **consensus_models.py** - TierManager with BandSelector integration
- **consensus_roles.py** - Domain-specific role assignments
- **consensus_synthesis.py** - Perspective aggregation engine
- **tests/test_consensus_models.py** - Unit tests for additive architecture

### What We Achieved:
- ✅ 71% API simplification (7 params → 2 required)
- ✅ 60% code reduction (4,000 → 1,600 lines)
- ✅ Additive tier architecture (verified by tests)
- ✅ BandSelector integration (no hardcoded models)
- ✅ Free model failover with caching
- ✅ Tool auto-registered in MCP
- ✅ Documentation (ADR, FORK_INVENTORY, TOOL_MATRIX)

### Current Limitation:
- Uses `_simulate_model_response()` placeholder
- No real API calls to models
- Workflow structure validated, API integration pending

---

## Phase 2: Real Model Integration

**Goal:** Replace simulated responses with actual model API calls

**Estimated Effort:** 8-12 hours
**Priority:** High (required for production use)

### Task 1: Model Provider Integration (4-6 hours)

**Objective:** Implement real model API calls using ModelProviderRegistry

**Current Code ([tiered_consensus.py:236](tools/custom/tiered_consensus.py#L236)):**
```python
# TODO: Actually call the model here
# For now, simulate model response
model_response = self._simulate_model_response(current_model, current_role, request.prompt)
```

**Implementation Steps:**

1. **Add ModelProviderRegistry dependency** (30 min)
   - Import ModelProviderRegistry in `__init__()`
   - Initialize provider registry reference
   - Add model resolution logic

2. **Create role-specific prompt builder** (1 hour)
   - Move from `create_role_prompt()` helper to method
   - Add context injection (files, images)
   - Handle domain-specific prompt variations

3. **Implement `_call_model()` method** (2 hours)
   ```python
   async def _call_model(
       self,
       model_name: str,
       role: str,
       prompt: str,
       files: List[str],
       images: List[str]
   ) -> str:
       """Call model with role-specific prompt."""
       # 1. Resolve model via ModelProviderRegistry
       # 2. Build role-specific system prompt
       # 3. Call provider.generate_content()
       # 4. Handle response
       # 5. Track cost
   ```

4. **Error handling and retry** (1 hour)
   - Handle API failures gracefully
   - Implement retry logic (max 3 attempts)
   - Log failures and continue with available models
   - Update synthesis to handle missing perspectives

5. **Cost tracking** (30 min)
   - Track actual API costs per model call
   - Aggregate total cost
   - Compare to estimated cost
   - Include in final report

6. **Testing** (1 hour)
   - Test with real API calls (small prompts)
   - Verify all 3 levels work correctly
   - Test error handling paths
   - Validate cost calculations

**Reference Implementation:**
- Check [tools/simple/base.py:444](tools/simple/base.py#L444) for `provider.generate_content()`
- Check [tools/consensus.py](tools/consensus.py) for multi-model consultation pattern

**Acceptance Criteria:**
- [ ] Real model API calls working for all 3 levels
- [ ] Error handling prevents workflow failure
- [ ] Cost tracking accurate
- [ ] No placeholder responses

---

### Task 2: Integration Tests (2-3 hours)

**Objective:** Test full consensus workflow with real/mocked models

**Test Coverage:**

1. **Basic Workflow Tests** (1 hour)
   - Level 1: 3 free models consultation
   - Level 2: 6 models (additive architecture)
   - Level 3: 8 models (additive architecture)
   - Verify synthesis generation

2. **Domain-Specific Tests** (45 min)
   - code_review domain
   - security domain
   - architecture domain
   - general domain

3. **Error Handling Tests** (45 min)
   - Model API failure (retry logic)
   - Partial model availability
   - Invalid model names
   - Network errors

4. **Cost Limit Tests** (30 min)
   - max_cost parameter enforcement
   - Cost tracking accuracy
   - Cost estimation validation

**Test File:** `tests/test_tiered_consensus_integration.py`

**Acceptance Criteria:**
- [ ] All 3 levels tested with real workflow
- [ ] Domain-specific role assignments verified
- [ ] Error paths covered
- [ ] Cost tracking validated

---

### Task 3: End-to-End MCP Testing (1-2 hours)

**Objective:** Test tool via MCP protocol with Claude client

**Test Scenarios:**

1. **Basic Consensus Request** (30 min)
   ```json
   {
     "prompt": "Should we migrate from PostgreSQL to MongoDB?",
     "level": 2
   }
   ```
   - Verify MCP protocol communication
   - Check workflow progression
   - Validate final synthesis

2. **Advanced Features** (30 min)
   ```json
   {
     "prompt": "Evaluate our microservices architecture",
     "level": 3,
     "domain": "architecture",
     "max_cost": 3.0
   }
   ```
   - Test domain-specific roles
   - Verify cost limit enforcement

3. **Edge Cases** (30 min)
   - Invalid level (0, 4)
   - Invalid domain
   - Missing required fields
   - Extremely long prompts

**Test Method:**
- Use `communication_simulator_test.py` framework
- Add tiered_consensus test case
- Validate MCP tool discovery
- Test actual model calls

**Acceptance Criteria:**
- [ ] Tool discovered via MCP protocol
- [ ] All parameters validated correctly
- [ ] Workflow completes successfully
- [ ] Results formatted properly

---

### Task 4: Performance Optimization (2-3 hours)

**Objective:** Optimize for production use

**Optimizations:**

1. **Parallel Model Calls** (1.5 hours)
   - Current: Sequential consultation
   - Goal: Parallel where possible
   - Challenge: Role-specific prompts differ
   - Solution: Use asyncio.gather() for independent calls

2. **Response Streaming** (1 hour)
   - Stream partial results as models respond
   - Progressive synthesis updates
   - Reduce perceived latency

3. **Caching** (30 min)
   - Cache identical prompt+model combinations
   - TTL-based cache (5 minutes)
   - Reduce API costs for testing

**Acceptance Criteria:**
- [ ] Level 3 (8 models) completes in < 30 seconds
- [ ] Streaming works for long analyses
- [ ] Cache reduces repeat query costs

---

## Phase 3: Advanced Features (Future)

**Not required for MVP, scheduled for later iteration**

### Domain Expansion
- Performance domain (performance_engineer, load_tester, profiler)
- DevOps domain (deployment_specialist, sre, platform_engineer)
- Data domain (data_engineer, analyst, scientist)
- UX domain (ux_researcher, designer, accessibility_expert)

### Enhanced Synthesis
- Disagreement analysis with voting weights
- Confidence scoring per perspective
- Actionable recommendations extraction
- Risk assessment aggregation

### Monitoring & Analytics
- Model performance tracking
- Cost analytics dashboard
- Quality metrics per tier level
- A/B testing framework for role prompts

---

## Implementation Timeline

### Week 1: Core Integration
- **Days 1-2:** Model Provider Integration (Task 1)
- **Day 3:** Integration Tests (Task 2)
- **Day 4:** MCP Testing (Task 3)
- **Day 5:** Bug fixes and refinement

### Week 2: Optimization
- **Days 1-2:** Parallel model calls
- **Day 3:** Response streaming
- **Day 4:** Caching implementation
- **Day 5:** Performance testing and tuning

### Week 3: Polish & Documentation
- **Days 1-2:** User documentation and examples
- **Day 3:** MCP tool catalog entry
- **Day 4:** Migration guide for old consensus users
- **Day 5:** Final testing and release prep

---

## Success Metrics

### Phase 2 Complete When:
- [ ] All simulated responses replaced with real API calls
- [ ] Integration tests pass with 80%+ coverage
- [ ] MCP protocol testing successful
- [ ] Tool works end-to-end with Claude client
- [ ] Performance targets met (< 30s for Level 3)
- [ ] Cost tracking accurate within 5%
- [ ] Error handling prevents workflow failures
- [ ] Documentation updated with real examples

### Quality Gates:
- Unit tests: 90%+ coverage
- Integration tests: All scenarios pass
- MCP tests: All edge cases handled
- Performance: < 30s for 8 models
- Error rate: < 1% for valid inputs

---

## Risk Mitigation

### Risk: Model API Rate Limits
**Mitigation:**
- Implement exponential backoff
- Add configurable delays between calls
- Use free model tier for testing

### Risk: API Cost Overruns
**Mitigation:**
- Strict max_cost enforcement
- Default Level 1 (free models only)
- Cost estimation before execution
- User confirmation for Level 3

### Risk: Model Availability Issues
**Mitigation:**
- Already implemented: Free model failover
- Graceful degradation (continue with available models)
- Clear error messages
- Fallback to cached results

### Risk: Performance Bottlenecks
**Mitigation:**
- Parallel model calls where possible
- Response streaming for perceived speed
- Caching for identical queries
- Timeout enforcement

---

## Dependencies

### Required Before Phase 2:
- ✅ Phase 1 complete (architecture, tests, docs)
- ✅ BandSelector functional
- ✅ ModelProviderRegistry available
- ✅ MCP server running

### External Dependencies:
- Model provider API keys configured
- Network access to model APIs
- Test budget for API calls (~$5 for comprehensive testing)

---

## Next Steps

**Immediate (Today):**
1. Create integration test file structure
2. Write integration tests with mocked model calls
3. Test workflow structure without API calls
4. Document example usage

**This Week:**
1. Implement real model API calls (Task 1)
2. Run integration tests with real models (Task 2)
3. Test via MCP protocol (Task 3)

**Next Week:**
1. Performance optimization (Task 4)
2. Documentation and examples
3. Migration guide

---

## Notes

### Why Phased Approach?
- **Phase 1 validated architecture** - Additive tiers, BandSelector integration, role system
- **Phase 2 adds production capability** - Real API calls required for actual use
- **Phase 3 adds polish** - Advanced features for enterprise users

### What's Already Working?
- Workflow orchestration
- Additive tier model selection
- Role-based prompt generation
- Synthesis engine
- Cost estimation
- Free model failover logic
- Unit tests verify architecture

### What Needs Real Implementation?
- Model API calls (currently simulated)
- Error handling with real API failures
- Cost tracking with actual costs
- Performance optimization with real latency

---

**This phased approach ensures we have a solid foundation (Phase 1) before adding complexity (Phase 2), then polish (Phase 3).**
